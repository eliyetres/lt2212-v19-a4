import os
import sys
import time
import pickle
import random
import argparse
from neural_network import NeuralNetwork
from utils import convert_time, readfile, split_data, create_ngram, split_data_features_labels
from vectorization import load_gensim_model, remove_words, get_vocabulary, generate_indices, gen_tri_vec_split, get_w2v_vectors, generate_translation_vectors
from neural_network import NeuralNetwork


def train_language_model(start, trainedmodelfile, p, r, b, target_trigrams, target_indices, vectors, layer_size, epochs):
    print("Training language model.")

    # If no model is incoming, create one:
    if not os.path.isfile(trainedmodelfile) or os.path.getsize(trainedmodelfile) <=0:

        print("File not found")
        print("Initializing an empty model")
        trigram_target_model = NeuralNetwork(p, r)
        startpoint = 0

        init_sample = random.sample(target_trigrams, b)

        # Initiate network weights from sample                                                                                                                                          
        X, Y = gen_tri_vec_split(init_sample, vectors, target_indices)

        # Train language model                                                                                                                                                          
        trigram_target_model.start(X, Y, layer_size, len(Y[0]))
        
    else:
        print("File found")
        with open(trainedmodelfile, 'rb') as mf:
            print("Loading model from file {}.".format(mf))
            content = pickle.load(mf)
            startpoint = content[0]
            trigram_target_model = content[1]
            
    print(startpoint)
        
    # For each batch...
    for i in range(startpoint, len(target_trigrams), b):
        print("Trigrams {} - {} out of {}".format(i, i+b, len(target_trigrams)))
        X, Y = gen_tri_vec_split(target_trigrams[i:i+b], vectors, target_indices)        
        trigram_target_model.train(X, Y, epochs)

        # ...write model to file
        print("Writing model to {}, having processed {} trigrams.".format(trainedmodelfile, i+b))
        with open(trainedmodelfile, 'wb') as tmf:
            pickle.dump([i+b, trigram_target_model], tmf)

    stop = time.time()
    hour, minute, second = (convert_time(start, stop))
    print("Trained {} sentences on {} hours, {} minutes and {} seconds".format(len(target_trigrams), hour, minute, second))
    print("A trigram model is saved to the file {}".format(trainedmodelfile))


def train_translation_model(start, trainedmodelfile, p, r, b, source_indices, source_train, target_train, vectors, layer_size, epochs):
    print("Training translation model.")

    #If no model is incoming, create one:                                                                                                                                                
    if not os.path.isfile(trainedmodelfile) or os.path.getsize(trainedmodelfile) <=0:

        print("File not found")
        print("Initiating an empty model")
        translation_model = NeuralNetwork(p, r)
        startpoint = 0

        start_i = random.randint(0,len(target_train)-b) # random start position                                                                                                              
        end_i = start_i+b

        X_list, Y_list = generate_translation_vectors(target_train[start_i:end_i], source_train[start_i:end_i], vectors, source_indices)
        
        # Train translation model
        translation_model.start(X_list, Y_list, layer_size, len(Y_list[0]))
    else:
        print("File found")
        with open(trainedmodelfile, 'rb') as mf:
            print("Loading model from file {}.".format(mf))
    
            content = pickle.load(mf)
            startpoint = content[0]
            translation_model = content[1]

    print(startpoint)
            
    # Train translation model
    for i in range(startpoint, len(target_train), b):
        print("Sentences {} - {} out of {}".format(i, i+b, len(target_train)))
        X_list, Y_list = generate_translation_vectors(target_train[i:i+b], source_train[i:i+b], vectors, source_indices)
        translation_model.train(X_list, Y_list, epochs)

        #...write model to file                                                                                                                                                         
        print("Writing model to {}, having processed {} sentences.".format(trainedmodelfile, i+b))
        with open(trainedmodelfile, 'wb') as tmf:
            pickle.dump([i+b, translation_model], tmf)    

    stop = time.time()
    hour, minute, second = (convert_time(start, stop))
    print("Ran {} sentences on {} hours, {} minutes and {} seconds".format(len(target_train), hour, minute, second))
    print("A translation model is saved to the file {}".format(trainedmodelfile))


parser = argparse.ArgumentParser(description="Feed forward neural networks.")

parser.add_argument("targetfile", type=str, default="UN-english-sample-small.txt", nargs='?', help="File used as target language.")
parser.add_argument("sourcefile", type=str, default="UN-french-sample-small.txt", nargs='?', help="File used as source language..")
parser.add_argument("modelfile", type=str, default="GoogleNews-vectors-negative300.bin", nargs='?', help="Pre-trained word2vec 300-dimensional vectors.")
parser.add_argument("trainedmodelfile", type=str, default="trained_translation_model", nargs='?', help="Trained language model")
parser.add_argument("-M", "--modeltype", metavar="M", dest="model_type", type=int, default=0, help="Choose whether to train translation (1) or trigram (0) model")
parser.add_argument("-B", "--batch", metavar="B", dest="batch", type=int,default=100, help="Batch size used for for training the neural network (default 100).")
parser.add_argument("-E", "--epoch", metavar="E", dest="epoch", type=int,default=20, help="Number or epochs used for training the neural network (default 20).")
parser.add_argument("-L", "--layer", metavar="L", dest="layer", type=int, default=600, help="Layer size for the neural network (default 600).")
parser.add_argument("-P", "--processor", metavar="P", dest="processor", type=str, default="cpu", help="Select processing unit (default cpu).")
parser.add_argument("-R", "--learning_rate", metavar="R", dest="learning_rate", type=float, default=0.01, help="Optimizer learning rate (default 0.01).")

args = parser.parse_args()

# Variables
test_size = 0.2 # Test data %, default 20
layer_size = args.layer
epochs = args.epoch
b = args.batch
p = args.processor
r = args.learning_rate
model_type = args.model_type

start = time.time()

print("Using {}.".format(args.processor))

print("Loading target language from {} and source language from {}.".format(args.targetfile, args.sourcefile))
target, source = readfile(args.targetfile, args.sourcefile)

print("Loading model from {}.".format(args.modelfile))
pre_trained_model = load_gensim_model(args.modelfile)

print("Removing words not found in the model.")
target_data, source_data = remove_words(target, source, pre_trained_model)

print("Splitting data into training and testing sets, {}/{}.".format(round(100-(test_size*100)), round(test_size*100)))
target_train, target_test, source_train, source_test = split_data(target_data, source_data, test_size)

if b >= len(target_train):
    exit("Error: training batch must be lower than training data.")

print("Generating vocabulary for target text.")
target_vocab = get_vocabulary(target_data)

print("Generating trigrams for target training data.")
target_trigrams = create_ngram(target_train)

print("Generating word indices.")
target_indices = generate_indices(target_vocab)

print("Fetching vectors from model.")
vectors = get_w2v_vectors(pre_trained_model, target_vocab)

print("Feeding training data in batches of size: {}".format(b))

# this means we're training trigram model
if model_type == 0:
    print("Generating vocabulary for target text.")
    target_vocab = get_vocabulary(target_data)

    print("Generating trigrams for target training data.")
    target_trigrams = create_ngram(target_train)

    print("Generating word indices.")
    target_indices = generate_indices(target_vocab)

    print("Fetching vectors from model.")
    vectors = get_w2v_vectors(pre_trained_model, target_vocab)

    print("Feeding training data in batches of size: {}".format(b))
    train_language_model(start, args.trainedmodelfile, p, r, b, target_trigrams, target_indices, vectors, layer_size, epochs)

# train translation model
else:
    print("Generating vocabulary for source text.")
    source_vocab = get_vocabulary(source_data)

    print("Generating vocabulary for target text.")
    target_vocab = get_vocabulary(target_data)

    print("Generating word indices.")
    source_indices = generate_indices(source_vocab)

    print("Fetching vectors from model.")
    vectors = get_w2v_vectors(pre_trained_model, target_vocab)

    print("Feeding training data in batches of size: {}".format(b))
    train_translation_model(start, args.trainedmodelfile, p, r, b, source_indices, source_train, target_train, vectors, layer_size, epochs)